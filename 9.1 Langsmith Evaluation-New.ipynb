{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42e15675-6368-4f08-91ca-d8b4ab4f9616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"{YOUR_LANGCHAIN_APIKEY}\"\n",
    "\n",
    "# Inputs are provided to your model, so it know what to generate\n",
    "dataset_inputs = [\n",
    "    \"a rap battle between Atticus Finch and Cicero\",\n",
    "    \"a rap battle between Barbie and Oppenheimer\",\n",
    "    # ... add more as desired\n",
    "]\n",
    "\n",
    "# Outputs are provided to the evaluator, so it knows what to compare to\n",
    "# Outputs are optional but recommended.\n",
    "dataset_outputs = [\n",
    "    {\"must_mention\": [\"lawyer\", \"justice\"]},\n",
    "    {\"must_mention\": [\"plastic\", \"nuclear\"]},\n",
    "]\n",
    "client = Client()\n",
    "dataset_name = \"Rap Battle Dataset\"\n",
    "\n",
    "# Storing inputs in a dataset lets us\n",
    "# run chains and LLMs over a shared set of examples.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Rap battle prompts.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in dataset_inputs],\n",
    "    outputs=dataset_outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71cc30cb-d0b2-4eb7-b12e-27423af77bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# You evaluate any arbitrary function over the dataset.\n",
    "# The input to the function will be the inputs dictionary for each example.\n",
    "def predict_result(input_: dict) -> dict:\n",
    "    messages = [{\"role\": \"user\", \"content\": input_[\"question\"]}]\n",
    "    response = openai.chat.completions.create(messages=messages, model=\"gpt-4o-mini\")\n",
    "    return {\"output\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5630e84-c01f-4675-86fe-934e3523bec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "from langsmith.evaluation.evaluator import EvaluationResult\n",
    "\n",
    "\n",
    "def must_mention_evaluator(run, example):\n",
    "    try:\n",
    "        prediction = run.outputs.get(\"output\", \"\")\n",
    "        required_phrases = example.outputs.get(\"must_mention\", [])\n",
    "        score = all(phrase.lower() in prediction.lower() for phrase in required_phrases)\n",
    "        return EvaluationResult(\n",
    "            key=\"must_mention\",\n",
    "            score=float(score),\n",
    "            comment=f\"Required phrases found: {score}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return EvaluationResult(\n",
    "            key=\"must_mention\",\n",
    "            score=0.0,\n",
    "            comment=f\"Error in evaluation: {str(e)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        RunEvalConfig.Criteria(\"helpfulness\"),\n",
    "        RunEvalConfig.Criteria(\"relevance\"),\n",
    "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
    "        RunEvalConfig.Criteria({\n",
    "            \"cliche\": \"Are the lyrics cliche? Respond Y if they are, N if they're entirely unique.\"\n",
    "        })\n",
    "    ],\n",
    "    custom_evaluators=[must_mention_evaluator]\n",
    ")\n",
    "\n",
    "evaluation_results = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=model,\n",
    "    evaluation=eval_config,\n",
    "    project_name=\"rap-battle-evaluation\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for result in evaluation_results:\n",
    "    print(f\"Result: {result}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
