{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e15675-6368-4f08-91ca-d8b4ab4f9616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "[Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langsmith/utils.py:87\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRap Battle Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Storing inputs in a dataset lets us\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# run chains and LLMs over a shared set of examples.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m dataset \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_dataset(\n\u001b[1;32m     27\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[1;32m     28\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRap battle prompts.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m client\u001b[38;5;241m.\u001b[39mcreate_examples(\n\u001b[1;32m     31\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: q} \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m dataset_inputs],\n\u001b[1;32m     32\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mdataset_outputs,\n\u001b[1;32m     33\u001b[0m     dataset_id\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langsmith/client.py:1785\u001b[0m, in \u001b[0;36mClient.create_dataset\u001b[0;34m(self, dataset_name, description, data_type)\u001b[0m\n\u001b[1;32m   1775\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ls_schemas\u001b[38;5;241m.\u001b[39mDatasetCreate(\n\u001b[1;32m   1776\u001b[0m     name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[1;32m   1777\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m   1778\u001b[0m     data_type\u001b[38;5;241m=\u001b[39mdata_type,\n\u001b[1;32m   1779\u001b[0m )\n\u001b[1;32m   1780\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1782\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_headers, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   1783\u001b[0m     data\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[1;32m   1784\u001b[0m )\n\u001b[0;32m-> 1785\u001b[0m ls_utils\u001b[38;5;241m.\u001b[39mraise_for_status_with_text(response)\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m   1787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[1;32m   1788\u001b[0m     _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[1;32m   1789\u001b[0m     _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tenant_id(),\n\u001b[1;32m   1790\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langsmith/utils.py:89\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     87\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHTTPError\u001b[0m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}"
     ]
    }
   ],
   "source": [
    "\n",
    "from langsmith import Client\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"{YOUR_LANGCHAIN_APIKEY}\"\n",
    "\n",
    "# Inputs are provided to your model, so it know what to generate\n",
    "dataset_inputs = [\n",
    "  \"a rap battle between Atticus Finch and Cicero\",\n",
    "  \"a rap battle between Barbie and Oppenheimer\",\n",
    "  # ... add more as desired\n",
    "]\n",
    "\n",
    "# Outputs are provided to the evaluator, so it knows what to compare to\n",
    "# Outputs are optional but recommended.\n",
    "dataset_outputs = [\n",
    "    {\"must_mention\": [\"lawyer\", \"justice\"]},\n",
    "    {\"must_mention\": [\"plastic\", \"nuclear\"]},\n",
    "]\n",
    "client = Client()\n",
    "dataset_name = \"Rap Battle Dataset\"\n",
    "\n",
    "# Storing inputs in a dataset lets us\n",
    "# run chains and LLMs over a shared set of examples.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Rap battle prompts.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in dataset_inputs],\n",
    "    outputs=dataset_outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a6d399-d660-476b-bb12-e623e3f7ccb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Rap Battle Dataset' description='Rap battle prompts.' data_type=<DataType.kv: 'kv'> id=UUID('8ee3fd99-5422-4bde-acba-cfc4a9f9f35d') created_at=datetime.datetime(2024, 2, 21, 8, 17, 6, 536983, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2024, 2, 21, 8, 17, 6, 536983, tzinfo=datetime.timezone.utc) example_count=0 session_count=0 last_session_start_time=None\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71cc30cb-d0b2-4eb7-b12e-27423af77bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "# You evaluate any arbitrary function over the dataset.\n",
    "# The input to the function will be the inputs dictionary for each example.\n",
    "def predict_result(input_: dict) -> dict:\n",
    "    messages = [{\"role\": \"user\", \"content\": input_[\"question\"]}]\n",
    "    response = openai.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\")\n",
    "    return {\"output\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5630e84-c01f-4675-86fe-934e3523bec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EvaluationResult(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust_mention\u001b[39m\u001b[38;5;124m\"\u001b[39m, score\u001b[38;5;241m=\u001b[39mscore)\n\u001b[1;32m     11\u001b[0m eval_config \u001b[38;5;241m=\u001b[39m RunEvalConfig(\n\u001b[1;32m     12\u001b[0m     custom_evaluators\u001b[38;5;241m=\u001b[39m[must_mention],\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# You can also use a prebuilt evaluator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     ],\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m client\u001b[38;5;241m.\u001b[39mrun_on_dataset(\n\u001b[1;32m     30\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[0;32m---> 31\u001b[0m     llm_or_chain_factory\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     32\u001b[0m     evaluation\u001b[38;5;241m=\u001b[39meval_config,\n\u001b[1;32m     33\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchatopenai-test-1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "\n",
    "@run_evaluator\n",
    "def must_mention(run, example) -> EvaluationResult:\n",
    "    prediction = run.outputs[\"generations\"][0][0][\"text\"]\n",
    "    required = example.outputs.get(\"must_mention\") or []\n",
    "    score = all(phrase in prediction for phrase in required)\n",
    "    return EvaluationResult(key=\"must_mention\", score=score)\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators=[must_mention],\n",
    "    # You can also use a prebuilt evaluator\n",
    "    # by providing a name or RunEvalConfig.<configured evaluator>\n",
    "    evaluators=[\n",
    "        # You can specify an evaluator by name/enum.\n",
    "        # In this case, the default criterion is \"helpfulness\"\n",
    "        \"criteria\",\n",
    "        # Or you can configure the evaluator\n",
    "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
    "        RunEvalConfig.Criteria(\n",
    "            {\n",
    "                \"cliche\": \"Are the lyrics cliche?\"\n",
    "                \" Respond Y if they are, N if they're entirely unique.\"\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=\"chatopenai-test-1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94233740-a307-4e4b-8e9c-49bffa8474a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
